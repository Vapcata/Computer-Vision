{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7VNgmJuv8kc_",
   "metadata": {
    "id": "7VNgmJuv8kc_"
   },
   "source": [
    "# Install all necessary modules here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "JFNtu2mM8sOR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JFNtu2mM8sOR",
    "outputId": "ee4d59e9-a95c-4876-a04b-5715fe614313"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /Users/alexandervaptsarov/anaconda3/envs/CV/lib/python3.9/site-packages (0.17.1)\n",
      "Requirement already satisfied: numpy in /Users/alexandervaptsarov/anaconda3/envs/CV/lib/python3.9/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.2.1 in /Users/alexandervaptsarov/anaconda3/envs/CV/lib/python3.9/site-packages (from torchvision) (2.2.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/alexandervaptsarov/anaconda3/envs/CV/lib/python3.9/site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: filelock in /Users/alexandervaptsarov/anaconda3/envs/CV/lib/python3.9/site-packages (from torch==2.2.1->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/alexandervaptsarov/anaconda3/envs/CV/lib/python3.9/site-packages (from torch==2.2.1->torchvision) (4.10.0)\n",
      "Requirement already satisfied: sympy in /Users/alexandervaptsarov/anaconda3/envs/CV/lib/python3.9/site-packages (from torch==2.2.1->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/alexandervaptsarov/anaconda3/envs/CV/lib/python3.9/site-packages (from torch==2.2.1->torchvision) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/alexandervaptsarov/anaconda3/envs/CV/lib/python3.9/site-packages (from torch==2.2.1->torchvision) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/alexandervaptsarov/anaconda3/envs/CV/lib/python3.9/site-packages (from torch==2.2.1->torchvision) (2024.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alexandervaptsarov/anaconda3/envs/CV/lib/python3.9/site-packages (from jinja2->torch==2.2.1->torchvision) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/alexandervaptsarov/anaconda3/envs/CV/lib/python3.9/site-packages (from sympy->torch==2.2.1->torchvision) (1.3.0)\n",
      "Collecting torchmetrics\n",
      "  Downloading torchmetrics-1.3.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>1.20.0 in /Users/alexandervaptsarov/anaconda3/envs/CV/lib/python3.9/site-packages (from torchmetrics) (1.26.4)\n",
      "Requirement already satisfied: packaging>17.1 in /Users/alexandervaptsarov/anaconda3/envs/CV/lib/python3.9/site-packages (from torchmetrics) (24.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/alexandervaptsarov/anaconda3/envs/CV/lib/python3.9/site-packages (from torchmetrics) (2.2.1)\n",
      "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
      "  Downloading lightning_utilities-0.11.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: setuptools in /Users/alexandervaptsarov/anaconda3/envs/CV/lib/python3.9/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (68.2.2)\n",
      "Requirement already satisfied: typing-extensions in /Users/alexandervaptsarov/anaconda3/envs/CV/lib/python3.9/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.10.0)\n",
      "Requirement already satisfied: filelock in /Users/alexandervaptsarov/anaconda3/envs/CV/lib/python3.9/site-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\n",
      "Requirement already satisfied: sympy in /Users/alexandervaptsarov/anaconda3/envs/CV/lib/python3.9/site-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/alexandervaptsarov/anaconda3/envs/CV/lib/python3.9/site-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/alexandervaptsarov/anaconda3/envs/CV/lib/python3.9/site-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/alexandervaptsarov/anaconda3/envs/CV/lib/python3.9/site-packages (from torch>=1.10.0->torchmetrics) (2024.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alexandervaptsarov/anaconda3/envs/CV/lib/python3.9/site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/alexandervaptsarov/anaconda3/envs/CV/lib/python3.9/site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
      "Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: lightning-utilities, torchmetrics\n",
      "Successfully installed lightning-utilities-0.11.2 torchmetrics-1.3.2\n",
      "Requirement already satisfied: torch in /Users/alexandervaptsarov/anaconda3/envs/CV/lib/python3.9/site-packages (2.2.1)\n",
      "Requirement already satisfied: filelock in /Users/alexandervaptsarov/anaconda3/envs/CV/lib/python3.9/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/alexandervaptsarov/anaconda3/envs/CV/lib/python3.9/site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in /Users/alexandervaptsarov/anaconda3/envs/CV/lib/python3.9/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/alexandervaptsarov/anaconda3/envs/CV/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/alexandervaptsarov/anaconda3/envs/CV/lib/python3.9/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/alexandervaptsarov/anaconda3/envs/CV/lib/python3.9/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alexandervaptsarov/anaconda3/envs/CV/lib/python3.9/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/alexandervaptsarov/anaconda3/envs/CV/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /Users/alexandervaptsarov/anaconda3/envs/CV/lib/python3.9/site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision\n",
    "!pip install torchmetrics\n",
    "!pip install torch\n",
    "!pip install numpy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bf9f74",
   "metadata": {
    "id": "d3bf9f74"
   },
   "source": [
    "# Preparation of milestone three\n",
    "\n",
    "Today we will start preparing the third milestone. The third milestone is to train an object detector to recognize cells. To successfully complete the milestone, you will have to complete the following sub-tasks:\n",
    "- Initialize a pytorch object detector. For this you can use the code provided. If you are not having a gpu available, please **freeze** tha backbone of your network as otherwise one forward pass will take to muchtime. With a frozenweights, you won't get as much performance as otherwise.\n",
    "- You will have to write a [training and validation/test](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html) loop to train your detector. Make sure you measure the convergence of the training by monitoring a detection metric like the [mAP](https://torchmetrics.readthedocs.io/en/stable/detection/mean_average_precision.html). Also, you will have to find a way to select the best model during training based on some metric.\n",
    "- You will have to train your model until convergence using the  class you created for the last milestone. Also you will have to pass your dataset to a dataloader to be able to use multithreading as well as automatic batching.\n",
    "- At the end, you will have to save the **state_dict** of your trained object detector, to be able to reuse it later.\n",
    "\n",
    "Please use a jupyter notebook for coding your training/testing pipeline. In the end, you will have to submit that jupiter notebook at moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418e17a1",
   "metadata": {
    "id": "418e17a1"
   },
   "source": [
    "# If you run the notebook in colab, you have to mount the google drive with the images. Proceed as follows:\n",
    "\n",
    "- **First**: Open the following **[link](https://drive.google.com/drive/folders/18P74V8kli6qDZtGBLN-tPrJFu3O2NPEK?usp=sharing)** in a new tab.\n",
    "- **Second**: Add a link to your google Drive.\n",
    "Example: [Link](https://drive.google.com/file/d/1IcFGGIoktPkDj9-4j5IQ3evInn0c2aq-/view?usp=sharing)\n",
    "- **Third**: Run the line of code below\n",
    "- **Fourth**: Grant Google access to your Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "brYRbF8idhWa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "brYRbF8idhWa",
    "outputId": "00b93bf5-1dc5-4953-e632-3921bf10f6a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# path to the link you created\n",
    "path_to_slides = '/content/gdrive/MyDrive/AgNORs/'\n",
    "# mount the data\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9CkOa6zxbj5Z",
   "metadata": {
    "id": "9CkOa6zxbj5Z"
   },
   "source": [
    "# 1. Initializing the model\n",
    "\n",
    "In this project, we will use a pre-trained RetinaNet model as the backbone for our object detection task. The model and weights can be easily loaded from the torchvision library. It is important to note that the anchor boxes used by the model may need to be adjusted to suit the specific task.\n",
    "\n",
    "The behavior of the RetinaNet model changes depending on whether it is in training or evaluation mode. During training, the model expects both an image and a dictionary of targets as input. It returns a dictionary containing the losses and predictions. During validation, the model only expects images as input and returns the predictions without calculating any losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "_YC7EQDffb92",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_YC7EQDffb92",
    "outputId": "b31d3223-8f33-48eb-8f01-cbfc341c66ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth\" to /Users/alexandervaptsarov/.cache/torch/hub/checkpoints/mobilenet_v2-7ebf99e0.pth\n",
      "100%|███████████████████████████████████████████| 13.6M/13.6M [00:18<00:00, 772kB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import RetinaNet\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from torchvision.models import MobileNet_V2_Weights\n",
    "\n",
    "# load a pre-trained model for classification and return\n",
    "# only the features\n",
    "backbone = torchvision.models.mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT).features\n",
    "\n",
    "# RetinaNet needs to know the number of\n",
    "# output channels in a backbone. For mobilenet_v2, it's 1280,\n",
    "# so we need to add it here\n",
    "backbone.out_channels = 1280\n",
    "# let's make the network generate 5 x 3 anchors per spatial\n",
    "# location, with 5 different sizes and 3 different aspect\n",
    "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
    "# map could potentially have different sizes and\n",
    "# aspect ratios\n",
    "anchor_generator = AnchorGenerator(\n",
    "     sizes=((32, 64, 128, 256, 512),),\n",
    "     aspect_ratios=((0.5, 1.0, 2.0),)\n",
    ")\n",
    "# put the pieces together inside a RetinaNet model\n",
    "model = RetinaNet(backbone,\n",
    "                  num_classes=2,\n",
    "                  anchor_generator=anchor_generator)\n",
    "\n",
    "######## uncomment these lines to freeze you network ############\n",
    "for p in model.backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "305f98bf",
   "metadata": {
    "id": "305f98bf"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import platform\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from numpy import random\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, annotations_frame,\n",
    "                 path_to_slides,\n",
    "                 crop_size = (128,128),\n",
    "                 pseudo_epoch_length:int = 1000,\n",
    "                 transformations = None):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        if platform.system() == 'Linux':\n",
    "            self.separator = '/'\n",
    "        else:\n",
    "            self.separator = '\\\\'\n",
    "\n",
    "\n",
    "        with open(\"/content/gdrive/MyDrive/annotation_frame.p\", 'rb') as f:\n",
    "              annotations_frame = pickle.load(f)\n",
    "        self.anno_frame=annotations_frame\n",
    "        \n",
    "        self.path_to_slides = path_to_slides\n",
    "        self.crop_size = crop_size\n",
    "        self.pseudo_epoch_length = pseudo_epoch_length\n",
    "        \n",
    "        # list which holds annotations of all slides in slide_names in the format\n",
    "        # slide_name, annotation, label, min_x, max_x, min_y, max_y\n",
    "        \n",
    "        self.slide_dict, self.annotations_list = self._initialize()\n",
    "        self.sample_cord_list = self._sample_cord_list()\n",
    "\n",
    "        # set up transformations\n",
    "        self.transformations = transformations\n",
    "        self.transform_to_tensor = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "\n",
    "    def _initialize(self):\n",
    "        # open all images and store them in self.slide_dict with their name as key value\n",
    "        slide_dict = {}\n",
    "        annotations_list = []\n",
    "        for slide in self.anno_frame.filename.unique():\n",
    "            # open slide\n",
    "            slide_dict[slide] =  Image.open(self.path_to_slides + self.separator + slide).convert('RGB')\n",
    "            im_obj = Image.open(self.path_to_slides + self.separator + slide).convert('RGB')\n",
    "            slide_dict[slide] = im_obj\n",
    "            # setting up a list with all bounding boxes\n",
    "            for idx,annotations in self.anno_frame[self.anno_frame.filename == slide][['max_x','max_y','min_x','min_y','label']].iterrows():\n",
    "                annotations_list.append([slide, annotations['label'], annotations['min_x'], annotations['min_y'], annotations['max_x'], annotations['max_y']])\n",
    "\n",
    "        return slide_dict, annotations_list\n",
    "\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        slide, x_cord, y_cord = self.sample_cord_list[index]\n",
    "        x_cord = np.int64(x_cord)\n",
    "        y_cord = np.int64(y_cord)\n",
    "        # load image\n",
    "        img = self.slide_dict[slide].crop((x_cord,y_cord,x_cord + self.crop_size[0],y_cord + self.crop_size[1]))\n",
    "        # transform image\n",
    "        #img = self.transformations(img)\n",
    "        \n",
    "        # load boxes for the image\n",
    "        labels_boxes = self._get_boxes_and_label(slide,x_cord,y_cord)\n",
    "        \n",
    "        labels_boxes = [[i[1] - x_cord, i[2] - y_cord, i[3] - x_cord, i[4] - y_cord] + [i[0]] for i in labels_boxes]\n",
    "        \n",
    "        \n",
    "        # applay transformations\n",
    "        if self.transformations != None:\n",
    "            if len(labels_boxes) > 0:\n",
    "                transformed = self.transformations(image = np.array(img), bboxes = labels_boxes)\n",
    "                boxes = torch.tensor([line[:-1] for line in transformed['bboxes']], dtype = torch.float32)\n",
    "                labels = torch.ones(boxes.shape[0], dtype = torch.int64)\n",
    "                img = self.transform_to_tensor(transformed['image'])\n",
    "                \n",
    "            # check if there is no labeld instance on the image\n",
    "            if len(labels_boxes) == 0:\n",
    "                labels = torch.tensor([0], dtype = torch.int64)\n",
    "                boxes = torch.zeros((0,4),dtype = torch.float32)\n",
    "                img = self.transform_to_tensor(img)\n",
    "\n",
    "        else:\n",
    "            if len(labels_boxes) == 0:\n",
    "                labels = torch.tensor([0], dtype = torch.int64)\n",
    "                boxes = torch.zeros((0,4),dtype = torch.float32)\n",
    "                img = self.transform_to_tensor(img)\n",
    "            else:\n",
    "                # now, you need to change the originale box cordinates to the cordinates of the image\n",
    "                boxes = torch.tensor([line[:-1] for line in labels_boxes],dtype=torch.float32)\n",
    "                labels = torch.ones(boxes.shape[0], dtype = torch.int64)\n",
    "                img = self.transform_to_tensor(img)\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels':labels\n",
    "        }\n",
    "\n",
    "        return img, target\n",
    "        \n",
    "\n",
    "    def _sample_cord_list(self):\n",
    "        # select slides from which to sample an image\n",
    "        slide_names = np.array(list(self.slide_dict.keys()))\n",
    "        slide_indice = random.choice(np.arange(len(slide_names)), size = self.pseudo_epoch_length, replace = True)\n",
    "        slides = slide_names[slide_indice]\n",
    "        # select coordinates from which to load images\n",
    "        # only works if all images have the same size\n",
    "        width,height = self.slide_dict[slides[0]].size\n",
    "        cordinates = random.randint(low = (0,0), high=(width - self.crop_size[0], height - self.crop_size[1]), size = (self.pseudo_epoch_length,2))\n",
    "        return np.concatenate((slides.reshape(-1,1),cordinates), axis = -1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.pseudo_epoch_length\n",
    "\n",
    "    def _get_boxes_and_label(self,slide,x_cord,y_cord):\n",
    "        return [line[1::] for line in self.annotations_list if line[0] == slide and line[2] > x_cord and line [3] > y_cord and line[4] < x_cord + self.crop_size[0] and line[5] < y_cord + self.crop_size[1]]\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Since each image may have a different number of objects, we need a collate function (to be passed to the DataLoader).\n",
    "        This describes how to combine these tensors of different sizes. We use lists.\n",
    "        Note: this need not be defined in this Class, can be standalone.\n",
    "        :param batch: an iterable of N sets from __iter__()\n",
    "        :return: a tensor of images, lists of varying-size tensors of bounding boxes, labels, and difficulties\n",
    "        \"\"\"\n",
    "\n",
    "        images = []\n",
    "        targets = []\n",
    "\n",
    "        for img, target in batch:\n",
    "            images.append(img)\n",
    "            targets.append(target)\n",
    "\n",
    "        images = torch.stack(images, dim=0)\n",
    "\n",
    "        return images, targets\n",
    "\n",
    "    def trigger_sampling(self):\n",
    "        \n",
    "        self.sample_cord_list = self._sample_cord_list()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ryYVXb8Hf-",
   "metadata": {
    "id": "c0ryYVXb8Hf-"
   },
   "source": [
    "# 2. Setting up an optimzer, a detection metric and the train and validation dataloaders\n",
    "\n",
    "To train the object detector, it is necessary to select an appropriate optimizer. Additionally, the torchmetrics class needs to be instantiated before it can be used for evaluation or tracking metrics during training.\n",
    "Additionally, initialize a training and validation dataloader your dataset. For more information on how to set up your dataloaders have a look [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4iNlBnCU8pX3",
   "metadata": {
    "id": "4iNlBnCU8pX3"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/gdrive/MyDrive/annotation_frame.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/content/gdrive/MyDrive/annotation_frame.p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/content/gdrive/MyDrive/AgNORs/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpseudo_epoch_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.8\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset))\n\u001b[1;32m     10\u001b[0m val_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset) \u001b[38;5;241m-\u001b[39m train_size\n",
      "Cell \u001b[0;32mIn[2], line 26\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, annotations_frame, path_to_slides, crop_size, pseudo_epoch_length, transformations)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseparator \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/content/gdrive/MyDrive/annotation_frame.p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     27\u001b[0m       annotations_frame \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manno_frame\u001b[38;5;241m=\u001b[39mannotations_frame\n",
      "File \u001b[0;32m~/anaconda3/envs/CV/lib/python3.9/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/MyDrive/annotation_frame.p'"
     ]
    }
   ],
   "source": [
    "# add your code\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "\n",
    "dataset = Dataset(\"/content/gdrive/MyDrive/annotation_frame.p\", '/content/gdrive/MyDrive/AgNORs/', crop_size=(128, 128), pseudo_epoch_length=1000, transformations=None)\n",
    "\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=dataset.collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=dataset.collate_fn)\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "\n",
    "metrics = torchmetrics.MetricCollection([\n",
    "    torchmetrics.Accuracy(num_classes=2,task='binary'),\n",
    "    torchmetrics.Precision(num_classes=2,task='binary'),\n",
    "    torchmetrics.Recall(num_classes=2,task='binary'),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I2s7huwv8s72",
   "metadata": {
    "id": "I2s7huwv8s72"
   },
   "source": [
    "#3. Train and validation loop\n",
    "\n",
    "Please write two functions, one for training and one for evaluating your object detector. Use these functions to train the detector for a few epochs. During training, track both the training losses and validation metrics to monitor the model's performance. Save the best detector as observerd by the validation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DniR_K0vmOnl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "DniR_K0vmOnl",
    "outputId": "599433ba-4d0f-4a44-d788-24b71177fdab"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "import torchvision\n",
    "\n",
    "def train(model, train_loader, optimizer, metrics, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for images, targets in train_loader:\n",
    "        images = tuple(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        total_loss += losses.item()\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader, metrics, device):\n",
    "    model.eval()\n",
    "    metric_results = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images = tuple(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Compute metrics\n",
    "            metrics(outputs, targets)\n",
    "        \n",
    "    for metric_name, metric in metrics.items():\n",
    "        metric_results[metric_name] = metric.compute()\n",
    "        metric.reset()\n",
    "    \n",
    "    return metric_results\n",
    "\n",
    "\n",
    "# Training loop\n",
    "best_metric = 0.0\n",
    "best_model = None\n",
    "num_epochs = 3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "metrics = torchmetrics.MetricCollection([\n",
    "    torchmetrics.Accuracy(num_classes=2, task='binary'),\n",
    "    torchmetrics.Precision(num_classes=2, task='binary'),\n",
    "    torchmetrics.Recall(num_classes=2, task='binary'),\n",
    "])\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, metrics, device)\n",
    "    val_metrics = evaluate(model, val_loader, metrics, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss:.4f}\")\n",
    "    print(\"Validation Metrics:\")\n",
    "    for metric_name, metric_value in val_metrics.items():\n",
    "        print(f\"{metric_name}: {metric_value:.4f}\")\n",
    "    \n",
    "    # Save the best model based on validation metric\n",
    "    if val_metrics['mAP'] > best_metric:\n",
    "        best_metric = val_metrics['mAP']\n",
    "        best_model = model.state_dict().copy()\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ivwojB899YXw",
   "metadata": {
    "id": "ivwojB899YXw"
   },
   "source": [
    "#4. Show some results and save your detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LSfIMQu0mRGd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 762
    },
    "id": "LSfIMQu0mRGd",
    "outputId": "dd0b221e-18d5-446d-e9c0-6d36053ac7c1"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "def visualize_results(model, dataloader, num_images=5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            images = images.to(device)\n",
    "            \n",
    "           \n",
    "            outputs = model(images)\n",
    "            \n",
    "         \n",
    "            pred_boxes = outputs[0]['boxes'].cpu()\n",
    "            pred_labels = outputs[0]['labels'].cpu()\n",
    "            \n",
    "          \n",
    "            true_boxes = targets[0]['boxes'].cpu()\n",
    "            true_labels = targets[0]['labels'].cpu()\n",
    "            \n",
    "         \n",
    "            for i in range(num_images):\n",
    "                img = F.to_pil_image(images[i].cpu())\n",
    "                plt.imshow(img)\n",
    "             \n",
    "                for box, label in zip(pred_boxes[i], pred_labels[i]):\n",
    "                    plt.rectangle((box[0], box[1]), (box[2], box[3]), edgecolor='r')\n",
    "                    plt.text(box[0], box[1], f'Label: {label.item()}', color='r')\n",
    "               \n",
    "                for box, label in zip(true_boxes[i], true_labels[i]):\n",
    "                    plt.rectangle((box[0], box[1]), (box[2], box[3]), edgecolor='g')\n",
    "                    plt.text(box[0], box[1], f'Label: {label.item()}', color='g')\n",
    "                    \n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "                \n",
    "            break  \n",
    "\n",
    "\n",
    "visualize_results(model, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9316321-568c-48b5-b861-5c8861f2685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_b0\n",
    "from torchviz import make_dot\n",
    "\n",
    "model = efficientnet_b0(weights='IMAGENET1K_V1')\n",
    "\n",
    "# list_models()\n",
    "# get_model_weights(efficientnet_b0)\n",
    "# output = \n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "    # print(name[-1])\n",
    "    # print(name, '-->', param.shape)\n",
    "# model.state_dict()\n",
    "# param_list = [*model.named_parameters()]\n",
    "# param_list[-2]\n",
    "# make_dot(yhat, params=dict(model.named_parameters()))\n",
    "model.classifier.requires_grad = True # --> torch.Size([1000, 1280])\n",
    "# classifier.1.bias #  --> torch.Size([1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8290ac71-438a-4a7c-a58d-ec317bc915ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "interpreter": {
   "hash": "cbc2ccb7899380f8e39344ba6fff5f38db9a0526ef90eff1147cd398f3029ddf"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
