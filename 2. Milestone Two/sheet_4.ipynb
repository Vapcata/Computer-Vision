{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of Milestone 2\n",
    "\n",
    "Today we will start to prepare the second milestone. The goals of this milestone are\n",
    "\n",
    "- Implement an image classification pipeline\n",
    "    - Implement a Pytorch classification dataset\n",
    "    - Implement at least two custom augmentations (do not just import one from Torchvision or albumentations)\n",
    "    - Implement a training and validation loop\n",
    "    - Optimize your model\n",
    "    - Evaluate the performance of your model with the F1 score on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Classification Dataset \n",
    "\n",
    "To train neural networks in pytorch with custom datasets, custom dataset classes are needed. Today we will work on a dataset for the term project. A general introduction to custom datasets can be found [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\n",
    "\n",
    "\n",
    "\n",
    "Your dataset must have several properties that are specific to the task:\n",
    "\n",
    "1. Since you only have a small number of images, you will need to use image augmentation. Image augmentation is always applied on the fly. **Never apply image augmentation to images and save the augmented versions to disk for sampling!** Normally, you can use the image augmentation implemented in albumentations or torchvision. **For this project, you will need to implement two augmentations yourself**. I would suggest implementing an augmentation that applies a Gaussian blur to the image and a function that applies a color jitter to the image. Implement the augmentations in a way that they can be **composed** with other TorchVision transformations.\n",
    "\n",
    "2. Apply minority class oversampling and majority class undersampling for better generalization. You can either implement an initialization function in your dataset where you first sample the coordinates for this epoch, or sample the coordinates on the fly in the __getitem__ method.\n",
    "\n",
    "3. Your dataset class will inherit from torch.utils.data.Dataset. But you must override the **__len__** and **__getitem__** functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_b0\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os \n",
    "from torchvision.io import read_image \n",
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler, Dataset\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "from sklearn.metrics import f1_score\n",
    "from torchvision.transforms import ToTensor, transforms, Resize\n",
    "import matplotlib.pyplot as plt\n",
    "import torchmetrics\n",
    "from torchmetrics import Accuracy\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform= None, target_transform= None): \n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir= img_dir\n",
    "        self.transform= transform\n",
    "        self.target_transform= target_transform\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        img_path= os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        img= Image.open(img_path).convert(\"RGB\")\n",
    "        label= self.img_labels.iloc[idx]\n",
    "        filename, max_x, max_y, min_x, min_y, label = self.img_labels.iloc[idx]\n",
    "        img = img.crop((min_x,min_y,max_x,max_y))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "        '''red = img[:, :, 0]\n",
    "        green = img[:, :, 1]\n",
    "        blue = img[:, :, 2]\n",
    "\n",
    "        new_img = np.zeros_like(img)\n",
    "        new_img[:, :, 0] = blue\n",
    "        new_img[:, :, 1] = red\n",
    "        new_img[:, :, 2] = green\n",
    "        jittered_img = np.clip(new_img * self.factor, 0, 255).astype(np.uint8)\n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "##Augmentations(Blur, color jitter)\n",
    "#class GaussianBlur:\n",
    " #   def __init__(self, radius=2):\n",
    "  #      self.radius = radius\n",
    "    #def __call__(self, img):\n",
    "      #  return img.filter(ImageFilter.GaussianBlur(self.radius))\n",
    "\n",
    "\n",
    "##ColourJitter\n",
    "#class ColorJitter:\n",
    " #   def __init__(self, factor=5):\n",
    "  #      self.factor = factor\n",
    "\n",
    "   # def __call__(self, img):\n",
    "    #    return self.apply_jitter(img)\n",
    "\n",
    "    #def apply_jitter(self, img):\n",
    "     #   red = img[:, :, 0]\n",
    "      #  green = img[:, :, 1]\n",
    "       # blue = img[:, :, 2]\n",
    "\n",
    "        #new_img = np.zeros_like(img)\n",
    "        #new_img[:, :, 0] = blue\n",
    "        #new_img[:, :, 1] = red\n",
    "       # new_img[:, :, 2] = green\n",
    "        #jittered_img = np.clip(new_img * self.factor, 0, 255).astype(np.uint8)\n",
    "        #return jittered_img\n",
    "\n",
    "\n",
    "class ImageRotator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def rotate(self, img):\n",
    "        rotated_img = img.rotate(90)\n",
    "        rotated_img.show()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/Validation/Test Split\n",
    "\n",
    "After you have successfully created your dataset, you need to define a training, validation, and test split of the data. Split the images at the image level! Splits must not overlap!\n",
    "\n",
    "1. Split the data.\n",
    "2. Initialize a dataset for each split and pass it to a Pytorch dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([Resize((224, 224)), ToTensor()])  \n",
    "\n",
    "dataset= CustomDataset(annotations_file='annotation_frame.csv', img_dir='Dataset',transform=transform, target_transform=None)\n",
    "dataset.transform= transform \n",
    "'''labels_map = [0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]\n",
    "dataset.transform = False\n",
    "\n",
    "figure = plt.figure(figsize=(10, 10))\n",
    "cols, rows = 5, 5\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(dataset.__len__(), size=(1,)).item()\n",
    "    img, label = dataset.__getitem__(sample_idx)\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "    \n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * dataset.__len__())\n",
    "eval_size = int(0.15 * dataset.__len__())\n",
    "test_size = dataset.__len__() - train_size - eval_size\n",
    "\n",
    "train_dataset, eval_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, eval_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "eval_loader = DataLoader(dataset=eval_dataset, batch_size=32)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initliazie a classification model from pytorch.\n",
    "\n",
    "For classification we will use a pre-trained model from pytorch. I would suggest an efficientnet_b0. Since we do not have too much training data and to mitigate overfitting, we will use ImageNet weights and only train the last layer. We will also freeze most of the network since we do not have much data and to save training time.\n",
    "\n",
    "1. Load the model from torchvision (see[https://pytorch.org/vision/stable/models.html]).\n",
    "2. Load the respective weights.\n",
    "3. Freeze all but the last layer.\n",
    "4. If you have a gpu available, bring your model to the gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the pre-trained model from torchvision\n",
    "model = efficientnet_b0(weights='DEFAULT')\n",
    "\n",
    "# Step 2: Freeze all but the last layer\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the last layer for training\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Step 3: Move the model to the GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Loop\n",
    "\n",
    "a) The next step is the actual model optimization. But before we implement the training and validation loops, we need to initialize a loss function and an optimizer. We can use Adam as the optimizer and Cross Entropy Loss as the loss function from pytorch. We also need to set a learning rate for the model optimization.\n",
    "\n",
    "1. Initialize a Loss Function\n",
    "2. Initializing an Optimizer and Setting a Learning Rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Now, we will implement an optimization loop. First we implement the training phase. Implement a function (**train_one_epoch**) which performs the follwing steps.:\n",
    "\n",
    "1. Define a variable running_loss in which you can store the change in loss during training. Since we will later chain the function with a validation function, bring the model to training mode if needed.\n",
    "2. Iterate over your train dataloader, in each epoch you must:\n",
    "    1. Transfer the tensors with the images and labels to the GPU or CPU, depending on the device on which your model is located.\n",
    "    2. Delete the old gradients in the optimizer.\n",
    "    3. Perform the actual forward pass, i.e. pass the images to the model and calculate the predictions (model(imgs))\n",
    "    4. Calculate the loss\n",
    "    5. Backpropagate the loss (loss.backward())\n",
    "    6. Perform the optimization step (optimizer.stepp())\n",
    "    7. Add the loss of the current batch to your running_loss\n",
    "    8. Print the loss every x batches, also return the average loss of the epoch\n",
    "\n",
    "**Tip:** use **[tqdm.notebook.tqdm](https://tqdm.github.io/docs/notebook/)** to visualize the progress during the epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, loss_fn, device):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    device = ('cpu')\n",
    "\n",
    "    for batch_idx, (img, labels) in enumerate(tqdm(train_loader, desc=\"Training iteration\")):\n",
    "        img, labels = img.to(device), labels.to(device)\n",
    "        # model.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(img)\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()* img.size(0)\n",
    "        \n",
    "        if batch_idx % 10 == 9: #  Print the loss every 10 batches\n",
    "            avg_loss = running_loss / 10\n",
    "            print(f'Batch {batch_idx + 1}/{len(train_loader)}, Loss: {avg_loss:.4f}')\n",
    "            running_loss = 0.0\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        return train_loss\n",
    "      \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) We also need a validation loop, which is called after a training loop to determine the training progress on the validation data. Implement this in a function (**validate_one_epoch**). This loop looks very similar to the training loop, but we can skip the backpropagation here. Also, use a metric from Torchmetrics to measure the performance of your models. The validation function should also return the loss or the metric so you can use it to select the best model during training.\n",
    "\n",
    "1. Turn off gradient storage. Put your model in evaluation mode. Since there is no backpropagation in the validation, we do not need it and can save a lot of memory.\n",
    "2. As with the training loop, iterate over the validation data loader and perform a forward pass in each epoch.\n",
    "3. Compute a loss in each epoch, but not backpropagated\n",
    "4. Store and output the loss and return the average loss of the epoch\n",
    "5. Update your metric with the predictions and labels of each batch\n",
    "6. Calculate your metric after the last batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def validate_one_epoch(model, eval_loader, loss_fn, metric, device):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "     \n",
    "    \n",
    "    \n",
    "    val_loss= 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (img, labels) in enumerate(tqdm(eval_loader, desc=\"Training iteration\")):\n",
    "            img, labels = img.to(device), labels.to(device)\n",
    "    \n",
    "\n",
    "            outputs = model(img)\n",
    "    \n",
    "            loss = loss_fn(outputs, labels)\n",
    "    \n",
    "            val_loss += loss.item() * img.size(0)\n",
    "            pred_labels = torch.argmax(outputs, dim=1)\n",
    "            metric.update(pred_labels, labels)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    val_loss /= len(eval_loader.dataset) \n",
    "    metric_result = metric.compute()\n",
    "    \n",
    "    return val_loss, metric_result\n",
    "\n",
    "num_classes = 12\n",
    "metric = Accuracy(task= 'MULTICLASS' , num_classes=num_classes)\n",
    "val_loss, val_metric = validate_one_epoch(model, eval_loader, loss_fn, device, metric)'''\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "def validate_one_epoch(model, eval_loader, loss_fn, device, metric):\n",
    "    model.eval()  \n",
    "    metric.reset()  \n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():  \n",
    "        for img, labels in eval_loader:\n",
    "            img, labels = img.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(img)\n",
    "            \n",
    "            loss = loss_fn(outputs, labels)\n",
    "            total_loss += loss.item()* img.size(0)\n",
    "            \n",
    "        \n",
    "            pred_labels = torch.argmax(outputs, dim=1)\n",
    "            metric.update(pred_labels, labels)\n",
    "    \n",
    "    avg_loss = total_loss / len(eval_loader.dataset)\n",
    "    \n",
    "    metric_value = metric.compute()\n",
    "    \n",
    "    print(f\"Validation Loss: {avg_loss}, Metric Value: {metric_value}\")\n",
    "    return avg_loss, metric_value\n",
    "\n",
    "num_classes = 12\n",
    "metric = Accuracy(task='multiclass', num_classes=num_classes)\n",
    "\n",
    "val_loss, val_metric = validate_one_epoch(model, eval_loader, loss_fn, device, metric)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Combine training and validation in an optimization loop. This loop is run **n_epochs** times. In each iteration, first call your training function and then your validation function. In each epoch, check to see if your model has improved, and if so, save the model. Do not save a new checkpoint in each epoch, as this will consume too much memory. Save the losses of the training and validation functions over the epochs to plot them at the end of your training. If you do not have a GPU available, you do not need to train your model to convergence. Just train a few steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization_loop(model, train_loader, eval_loader, loss_fn, optimizer, device, n_epochs, model_save_path=None):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}\")\n",
    "\n",
    "       \n",
    "        train_loss = train_one_epoch(model, train_loader, loss_fn, device)\n",
    "        train_losses.append(train_loss)\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        \n",
    "        val_loss, val_metric = validate_one_epoch(model, eval_loader, loss_fn, device, metric)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Metric Value: {val_metric}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "        \n",
    "            if model_save_path:\n",
    "                torch.save(model.state_dict(), model_save_path)\n",
    "                print(\"Model saved.\")\n",
    "  \n",
    "    plt.plot(range(1, n_epochs + 1), train_losses, label='Training Loss')\n",
    "    plt.plot(range(1, n_epochs + 1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "n_epochs = 10 \n",
    "model_save_path = 'best_model.pth'  # Path to save the best model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimization_loop(model, train_loader, eval_loader, loss_fn, optimizer, device, n_epochs, model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model testing\n",
    "\n",
    "Once the optimization is complete, you can test your model on the test data. This means that you apply your model to the images in your dataset. To do this, iterate over all instances in your test data set.\n",
    "\n",
    "1. Initialize the best performing model.\n",
    "2. Initialize the F1 score metric from TorchVision.\n",
    "3. Iterate over all instances in the test dataset and update your metric after each batch.\n",
    "4. Compute the final metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m  \n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      6\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(test_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTesting\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model_path = 'best_model.pth'  \n",
    "\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "progress_bar = tqdm(test_loader, desc='Testing')\n",
    "\n",
    "metric = F1Score(task='multiclass', num_classes=12).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for b, (inputs,labels) in enumerate(progress_bar):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        metric.update(outputs, labels)\n",
    "\n",
    "metric_value = metric.compute()\n",
    "print(f\"F1 score: {metric_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
