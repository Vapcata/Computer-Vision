{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation for Milestone Four\n",
    "\n",
    "Today, we will begin preparing for the final milestone. Here, we will assemble all the pieces of the pipeline you've created. You will need to write a function **compute_AgNOR_score**. This function first utilizes the detection model to locate cells within a given image and then feeds those cells into a classification model to classify them into one of the AgNOR classes. Finally, you will aggregate all predictions into a final AgNOR score for the entire image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Write a function \"process_image\" which receives an image and runs the detection model on it.\n",
    "\n",
    "The function should have the following parameters:\n",
    "\n",
    "1. image: The image on which you want to run inference.\n",
    "2. crop_size: The size of the crops you want to load from the imagea\n",
    "3. overlap: Percentage or number of pixels the crops should overlap.\n",
    "4. model: The object detection model. This function should generally be able to run with any detection model.\n",
    "5. detection_threshold: A threshold to apply to the detections to reject false positives.\n",
    "\n",
    "The function will have to tile the image into **overlapping crops** and then feed each crop to the model. After that, all detections have to be transformed to the global coordinate system of the image since the detections are within the coordinate system of the image crop. Subsequently, [non-maximal suppression](https://pytorch.org/vision/stable/generated/torchvision.ops.nms.html) needs to be applied to the detections in order to reject overlapping detections. In the end, the function will return the coordinates and scores of the detected cells that exceed the given threshold. Use **torch_no_grad** to save computation time and also ensure your **model is in evaluation mode** before feeding the cells to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'RetinaNet' from 'torchvision.models.detection.anchor_utils' (/Users/alexandervaptsarov/Library/jupyterlab-desktop/envs/cv/lib/python3.12/site-packages/torchvision/models/detection/anchor_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01malbumentations\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mA\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01malbumentations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ToTensorV2\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdetection\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manchor_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnchorGenerator, RetinaNet\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdetection\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfaster_rcnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastRCNNPredictor\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MobileNet_V2_Weights\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'RetinaNet' from 'torchvision.models.detection.anchor_utils' (/Users/alexandervaptsarov/Library/jupyterlab-desktop/envs/cv/lib/python3.12/site-packages/torchvision/models/detection/anchor_utils.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "import torch\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator, RetinaNet\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models import MobileNet_V2_Weights\n",
    "from torchvision.ops import nms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_retinanet(path_to_weights:str):\n",
    "    backbone = torchvision.models.mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT).features\n",
    "    backbone.out_channels = 1280\n",
    "    anchor_generator = AnchorGenerator(\n",
    "     sizes=((32, 64, 128, 256, 512),),\n",
    "     aspect_ratios=((0.5, 1.0, 2.0),)\n",
    ")\n",
    "    model = RetinaNet(backbone,\n",
    "                  num_classes=2,\n",
    "                  anchor_generator=anchor_generator)\n",
    "    \n",
    "    model.load_state_dict(torch.load(path_to_weights, map_location=device))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'retinanet/best_model_0.743_map.pth'\n",
    "detection_model = make_retinanet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'detection_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdetection_model\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'detection_model' is not defined"
     ]
    }
   ],
   "source": [
    "detection_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image, crop_size, overlap, model, detection_threshold):\n",
    "    image = torchvision.transforms.functional.to_tensor(image).to(device)\n",
    "\n",
    "    stride_height = crop_size[0] - overlap[0]\n",
    "    stride_width = crop_size[1] - overlap[1]\n",
    "\n",
    "    crops = []\n",
    "    for i in range(0, image.shape[1] - crop_size[0] + 1, stride_height):\n",
    "        for j in range(0, image.shape[2] - crop_size[1] + 1, stride_width):\n",
    "            crops.append(image[:, i:i + crop_size[0], j:j + crop_size[1]])\n",
    "\n",
    "    # Convert list to tensor\n",
    "    crops = torch.stack(crops)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(crops)\n",
    "\n",
    "    all_boxes = []\n",
    "    all_scores = []\n",
    "    for k, output in enumerate(outputs):\n",
    "        boxes = output['boxes']\n",
    "        scores = output['scores']\n",
    "\n",
    "        mask = scores > detection_threshold\n",
    "        boxes, scores = boxes[mask], scores[mask]\n",
    "\n",
    "        crop_row, crop_col = k // ((image.shape[2] - crop_size[0]) // stride_height + 1), k % ((image.shape[2] - crop_size[1]) // stride_width + 1)\n",
    "        boxes[:, [0, 2]] += crop_col * stride_height\n",
    "        boxes[:, [1, 3]] += crop_row * stride_width\n",
    "\n",
    "        all_boxes.append(boxes)\n",
    "        all_scores.append(scores)\n",
    "\n",
    "    all_boxes = torch.cat(all_boxes, dim=0)\n",
    "    all_scores = torch.cat(all_scores, dim=0)\n",
    "\n",
    "    # non-maximal suppression\n",
    "    keep = nms(all_boxes, all_scores, 0.1)  # Adjust the NMS threshold as needed\n",
    "\n",
    "    final_boxes = all_boxes[keep]\n",
    "    final_scores = all_scores[keep]\n",
    "\n",
    "    high_conf_mask = final_scores > detection_threshold\n",
    "    final_boxes = final_boxes[high_conf_mask]\n",
    "    final_scores = final_scores[high_conf_mask]\n",
    "\n",
    "    return final_boxes.cpu().numpy(), final_scores.cpu().numpy()\n",
    "\n",
    "# img = Image.open('path_to_your_image.jpg')\n",
    "# model.eval()\n",
    "# boxes, scores = process_image(img, crop_size=512, overlap=100, model=model, detection_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Write a function \"process_cells\" which classifies the cells from the coordinates that were given to the model.\n",
    "\n",
    "The function should have the following parameters:\n",
    "\n",
    "1. image: The image from which to load the cells.\n",
    "2. coords: Coordinates of the cells which you found with the detection algorithm.\n",
    "3. model: The trained classification model.\n",
    "4. crop_size: A size to resize the crops to. It should be equal to the size with which you trained the classification network.\n",
    "\n",
    "The function should load each cell from the respective image and feed them to the classification model. Save the prediction and, in the end, aggregate the classifications of all cells into a final AgNOR score. The function should return the labels of the respective cells as well as the final AgNOR score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "def process_cells(image, coords, model, crop_size):\n",
    "    model.eval()\n",
    "    # store pred\n",
    "    cell_labels = []\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        for box in coords:\n",
    "            box = box.tolist()\n",
    "            cell_image = image.crop((box[0], box[1], box[2], box[3]))\n",
    "            \n",
    "            cell_image = cell_image.resize(crop_size)\n",
    "            cell_image = torch.tensor(np.array(cell_image)).permute(2, 0, 1).float() / 255.0\n",
    "            cell_image = cell_image.unsqueeze(0) \n",
    "\n",
    "            # classification model\n",
    "            output = model(cell_image)\n",
    "            predicted_label = output.argmax(1).item() \n",
    "            cell_labels.append(predicted_label)\n",
    "\n",
    "    # final AgNOR score\n",
    "\n",
    "    if cell_labels:\n",
    "        final_agnor_score = sum(cell_labels) / len(cell_labels)\n",
    "    else:\n",
    "        final_agnor_score = 0\n",
    "    \n",
    "    return cell_labels, final_agnor_score\n",
    "\n",
    "\n",
    "# img = Image.open('path_to_image.jpg').convert(\"RGB\")\n",
    "# detected_coords = torch.tensor([[10, 10, 50, 50], [60, 60, 100, 100]])  # Example coordinates\n",
    "# classification_model = your_trained_classification_model\n",
    "# labels, agnor_score = process_cells(img, detected_coords, classification_model, (128, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Combine both functions into the function **compute_AgNOR_score**.\n",
    "\n",
    "This function should receive the image as a parameter and also require all parameters to execute the subfunctions. In the end, this function should return the overall AgNOR score of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_AgNOR_score(image, detection_model, classification_model, crop_size_detection, crop_size_classification, overlap, detection_threshold):\n",
    "    detected_boxes, _ = process_image(image, crop_size_detection, overlap, detection_model, detection_threshold)\n",
    "\n",
    "    if len(detected_boxes) > 0:\n",
    "        cell_labels, agnor_score = process_cells(image, detected_boxes, classification_model, crop_size_classification)\n",
    "    else:\n",
    "        cell_labels = []\n",
    "        agnor_score = 0  # No cells detected, hence no score\n",
    "\n",
    "    return agnor_score\n",
    "\n",
    "# detection_model = your_pretrained_detection_model\n",
    "# classification_model = your_pretrained_classification_model\n",
    "# image_path = 'path_to_your_image.jpg'\n",
    "# img = Image.open(image_path).convert(\"RGB\")\n",
    "# agnor_score = compute_AgNOR_score(img, detection_model, classification_model, (256, 256), (128, 128), (50, 50), 0.5)\n",
    "# print(f\"The AgNOR score of the image is {agnor_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import efficientnet_b0\n",
    "\n",
    "def make_imagenet(weights_path):\n",
    "\n",
    "    weights = 'IMAGENET1K_V1'\n",
    "    model = efficientnet_b0(weights=weights)\n",
    "\n",
    "    model.classifier[1] = nn.Linear(1280, 12)\n",
    "    #{\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'classifier' not in name:\n",
    "            param.requires_grad = False\n",
    "    \n",
    "\n",
    "    for name, param in model.classifier.named_parameters():\n",
    "        param.requires_grad = True\n",
    " \n",
    "    model.load_state_dict(torch.load(weights_path, map_location=device), strict=False)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "classification_model = make_imagenet('best_model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Test your pipeline.\n",
    "\n",
    "Take several images (approximately 5) and run them through your pipeline. Then, calculate the error between the predicted AgNOR score and the AgNOR score defined by the labels of the cells in the annotation file. To obtain this label, simply calculate the mean of the labels of the respective image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 13.975855341858315\n",
      "Actual Scores: [1.3578643578643579, 1.0689149560117301, 4.3623529411764705, 1.6595492289442468, 1.4829268292682927, 1.08364312267658, 1.051643192488263, 0.9709465791940018, 1.4591439688715953, 1.0815286624203821, 1.325179856115108, 1.3300536672629697, 1.0875912408759123, 1.1523972602739727, 0.6959677419354838, 1.567956795679568, 1.2726008344923505, 1.1495327102803738, 1.8744460856720828, 1.4244482173174873, 1.2068095838587642, 3.779874213836478, 2.112208892025406]\n",
      "Predicted Scores: [5.337209302325581, 4.989247311827957, 5.405063291139241, 5.08421052631579, 4.542124542124542, 6.2727272727272725, 5.553571428571429, 5.462837837837838, 4.362745098039215, 4.678571428571429, 5.537383177570093, 5.775462962962963, 6.487012987012987, 4.340425531914893, 5.772334293948127, 4.512820512820513, 5.8768115942028984, 5.037037037037037, 5.567164179104478, 3.5128205128205128, 4.644628099173554, 4.378378378378378, 3.367816091954023]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def test_pipeline(annotation_file, img_dir, detection_model, classification_model, crop_size_detection, crop_size_classification, overlap, detection_threshold):\n",
    "\n",
    "    annotations = pd.read_csv(annotation_file)\n",
    "    unique_images = annotations['filename'].unique()\n",
    "\n",
    "    predicted_scores = []\n",
    "    actual_scores = []\n",
    "\n",
    "    for image_name in unique_images:\n",
    "\n",
    "        image_path = os.path.join(img_dir, image_name)\n",
    "        if os.path.exists(image_path) and os.path.getsize(image_path) > 0:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            \n",
    "            # predict AgNOR score \n",
    "            predicted_score = compute_AgNOR_score(image, detection_model, classification_model, crop_size_detection, crop_size_classification, overlap, detection_threshold)\n",
    "            predicted_scores.append(predicted_score)\n",
    "            \n",
    "            # get actual score\n",
    "            labels = annotations[annotations['filename'] == image_name]['label']\n",
    "            actual_score = labels.mean()\n",
    "            actual_scores.append(actual_score)\n",
    "        else:\n",
    "            continue\n",
    "    # mean squared error\n",
    "    mse = mean_squared_error(actual_scores, predicted_scores)\n",
    "    return mse, actual_scores, predicted_scores\n",
    "\n",
    "\n",
    "mse, actual_scores, predicted_scores = test_pipeline(\n",
    "    'annotation_frame.csv',\n",
    "    'Dataset/',\n",
    "    detection_model,\n",
    "    classification_model,\n",
    "    (256, 256),\n",
    "    (128, 128),\n",
    "    (50, 50),\n",
    "    0.5\n",
    ")\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(\"Actual Scores:\", actual_scores)\n",
    "print(\"Predicted Scores:\", predicted_scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
